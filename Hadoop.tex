\chapter{Hadoop分布式文件系统}

\ECUSTabstract Hadoop分布式文件系统（HDFS，Hadoop Distributed File System）设计用于可靠存储非常大量的数据集，并将这些数据集以流的形式通过高速带宽发送的用户的应用。在大的集群中，成千台服务器同时直接拥有附加的存储空间和执行用户应用程序的任务。通过分布式存储和多台服务器交叉计算，资源可以根据需求增长，并且对于每种规模的资源需求都很经济划算。本文我们描述HDFS的架构和使用HDFS来管理Yahho! 25petabyte的企业数据感受。

\ECUSTkeywords Hadoop，HDFS，分布式文件系统


\section{引言和相关工作}

Hadoop提供了一个分布式文件系统，通过使用MapReduce模范分析和转化超大型数据集的框架。Hadoop一个重要的特点是分割数据且通过多台（上千台）主机进行交叉计算，且并行执行应用数据相关的计算。一个Hadoop集群只要简单地增加通常的服务器就能调整计算能力，存储能力和输入输出带宽。Yahoo!的Hadoop集群有25000台服务器，且存储着25 petabyte的应用程序数据，并且当中最大的集群拥有3500台服务器。世界上已经有100个其他的组织在使用Hadoop。

Hadoop是一个Apache的项目；所有的构成都可通过Apache开源许可获得。Yahoo!已经开发和贡献了Hadoop核心部分（HDFS和MapReduce）的80\%。HBase起初由Powerset开发，现在Powerset已经是Microsoft的一个部门。Hive起源和开发于Facebook。Pig、ZooKeeper和Chukwa起源和开发于Yahoo!。Avro起源于Yahoo!且和Cloudera共同开发。

HDFS是Hadoop的文件系统部分。虽然HDFS仿造了UNIX的文件系统的接口，但为了提高将来的应用程序的性能而牺牲了标准化接口。

Hadoop将文件系统的元数据和应用程序数据分开存储。正如其他分布式文件系统，比如PVFS、Lustre和GFS一样,HDFS将元数据存储在一台特定的服务器上，称为主节点（NameNode）。应用程序数据存储在其他服务器上，称为数据节点（DataNode）。所有服务器相互连接，并通过基于TCP的协议进行通信。

不像Lusre和PVFS，HDFS的主节点不使用如RAID这样的数据保护机制来保持数据的持久性。取而代之，它像GFS一样，为了可靠性，将文件的内容被复制到多个主节点上。在确保了数据可靠性的同时，这种策略有提高数据传输带宽的优势，并且有更多的机会来定位所需数据附近的计算资源。

多个分布式系统应经或是正在探索命名空间的真正分布式实现。Ceph拥有命名空间服务器集群（MDS），且使用动态子树分割算法来平衡地映射命名空间树到MDS。GFS也设计分布式命名空间实现。新的GFS拥有成百台命名空间服务器(宿主)，每台宿主服务器上有1亿个文件。Lusre，在Lusre2.2版的路标中拥有一个集群命名空间的实现。目地是为了跨多个元数据服务器维护一个目录，每个元数据服务器中包含着命名空间的一部分。一个文件根据文件名通过哈希函数（hash function）被分派给一个特定的MDS。

\section{架构}

\subsection{主节点}

HDFS命名空间是一种文件和目录的层次结构。文件和目录在主节点上通过inodes表示，inodes记录了诸如许可、修改和访问时间、命名空间和磁盘空间容量之类的属性。文件的内容被分割成大的块（典型的是128megabyte每块，but
user selectable file-by-file）且文件的各个块独立地复制到多个主节点上（典型的是三个，but user selectable file-by-file）。主节点维护命名空间树和文件块到数据节点（文件数据的物理位置）的映射。一个HDFS客户端想要读取第一个包含主节点的文件以获得包含文件的数据块的位置，然后从最靠近客户端的数据节点读取块的内容。当写数据时，客户端请求主节点分配一套三个节点来托管块的副本。然后客户端通过管道将数据写到数据节点。当前的设计是每个集群都有一个主节点。集群中可能拥有成千个数据节点和上万个HDFS客户端，这是因为一个数据节点可能同时运行多个应用程序任务。

HDFS把整个命名空间保存在内存中。inode数据和构成名字系统的元数据的各个文件的包含块列表统称为映像。保存在本地主机的本地文件系统中的映像的持久记录称为记录点。主节点还保存的映像的修改日志称为在本地主机的本地文件系统中的日记。为了提高持久性，多余的记录点和日记的副本可能由其他服务器产生。在重启过程中，主节点通过读取命名空间和重播日记恢复命名空间。块副本的位置可能随时间而改变，且不作为永久记录点的一部分。

\subsection{数据节点}

每个数据节点上的块副本由两个在本地主机的本地文件系统中的文件表示。第一个文件包含数据本身，第二个文件是块的元数据，包括块数据的校验和和块的generation stamp。数据文件的大小等于数据块的实际长度，且不需要额外的空间来round it up到像传统文件系统中名义上的块。因此，如果一个块是半满的，它只需在本地分区中占用全满时空间的一半。

在开启过程中，各个节点连接上主节点并进行握手。握手的目的是为了校验命名空间的ID和数据节点的软件版本。如过二者之一和主节点不匹配，则数据节点自动关闭。

命名空间ID在格式化的时候就分配给文件系统实例。命名空间ID永久存放在集群的所有节点中。不同命名空间ID的节点无法将加入这个集群，这样就保证了文件系统的完整性。

软件版本的一致性是很重要的，因为不匹配的版本可能导致数据出错或丢失，而且在数以千记的机器组成的大集群中，很容易忽视那些没有正常关闭的节点的优先级高于软件升级的优先级，或在升级过程中不可用。

刚被初始化且没有任何命名空间ID的数据节点，将被允许加入到集群中且获得集群的命名空间ID。

在握手之后，主节点注册数据节点，数据节点永久保存他们唯一的存储ID。存储ID是一个数据节点的内部标记，就算重启后使用不同的IP地址或是端口号，数据节点也能被识别。存储ID在数据节点第一次注册到主节点的时候分配，并且不再改变。

一个数据节点通过发送块报告，向主节点确认它所拥有的块副本。一个块报告包含块的ID，generation stamp和服务器主机上每个块副本长度。第一个块报告在数据节点注册之后立即发送。后续的块报告每小时发送一次，向主节点提供最新的块副本在集群中的位置信息。

在常规的操作当中，数据节点向主节点发送心跳（heartbeats）来确认数据节点正在运行且它所拥有的块副本可用。默认的心跳间隔是3秒。如果10分钟内主节点没收到来自数据节点的心跳，则主节点就认为数据节点已经失去服务且数据节点所拥有块副本不可用。之后主节点则计划在其他数据节点上产生新的块副本。

来自一个数据节点的心跳还包含着总存储容量、使用的存储碎片和当前正在传输的数据量的信息。这些主节点用根据这些统计来分配空间和实现负载平衡。

主节点并不直接控制数据节点。它使用答复心跳的方式来向数据节点发送指令。这些指令包括用于：
\begin{itemize}
\item复制块到其他节点
\item删除本地块副本
\item重新注册或关闭节点
\item立即发送块报告
\end{itemize}	
的命令。

这些命令对于维护整个系统的统一性很重要，因此甚至是对大的集群，保持心跳的频率是至关重要的。主节点可以每秒处理成千次心跳，却不影响其他主节点的操作。

\subsection{HDFS客户端}

用户应用程序使用HDFS客户端访问文件系统，HDFS客户端是一个输出HDFS文件系统接口的代码库。

类似于通常的文件系统，HDFS支持读、写和删除文件的操作和创建、删除目录的操作。用户在命名空间中通过路径引用文件和目录。用户应用程序通常不需要知道文件系统的元数据和存储分布在不同的服务器上，或是块有多个副本。

当一个应用程序读取一个文件时，HDFS客户端首先向请求主节点有文件块副本的数据节点列表。然后和一个数据节点直接通信并请求传送想要的块。当客户端写文件时，首先向主节点请求选择一个数据节点来保存文件的第一个块副本。客户端建立一个点到点的管道并发送数据。当第一个数据块充满后，客户端请求新的数据节点来保存下一个块。建立一个新的管道，客户段发送文件的下一部分。每次选择的数据节点可能都是不同的。客户端、主节点和数据节点的交互关系如图1所示。

不像通常的文件系统那样，HDFS提供一个显示文件块位置的API。这允许像MapReduce框架的应用程序能计划将数据放置于何处的任务，这样改进了读取的性能。这也允许应用程序设置文件的复制因子。默认的文件复制因子是3。对于至关重要的文件或是经常访问的数据，设置更大的复制因子能提高容错性和增加读带宽。

\subsection{映像和日记}

命名空间映像是文件系统中的元数据，它描述了了应用程序数据的组织形式，是目录还是文件。一个持久印象写在磁盘中的持久记录称为记录点。日记是一个事先写好提交的日志，日记记录了文件系统的永久性修改。对于每个客户端的初始化处理中，修改记录在日记中，且日记文件在提交修改到HDFS客户端前会刷新和同步。记录点文件永远不会被主节点改变；当在重启时，管理员或是将在下节中提到记录点节点要求创建一个新的记录点时，将会全部替换覆盖。在启动时，主节点从记录点初始化命名空间印象，然后从日记中重新执行修改直到映像是最新的，文件系统最后的状态。一个新的记录点和空日记在主节点开始服务客户端前被写回到存储目录。

如果记录点或日记丢失或出错，命名空间的部分或全部信息将会丢失。为了保留关键信息，HDFS可以被设置为在多个存储目录保存记录点和日记。推荐的方法是将目录放在不同的卷上，并且将一个目录放在远端的NFS服务器。第一个选择避免一个卷的失效引起数据丢失，第二个选择保护数据免受全部节点失效的影响。如果主节点在将日记写到存储目录之一时发生错误，它将自动把那个目录从存储目录列表中排除。当没有存储目录可用时，主节点将自动关闭。

主节点是多线程系统，并且同时处理来自多个客户端的请求。节约磁盘事务成为了瓶颈，因为所有其他线程需要等待直到由其中一个线程开启的同步刷新和同步的过程完成。为了优化这个过程，主节点批处理由不同客户端开启的多个事务。当有一个主节点的线程开始刷新和同步操作时，那时所有批处理事务都一起提交。只留下需要检查事务已经被保存且不需要开始刷新和同步操作的进程。

\subsection{检查点节点}

HDFS的主节点，除了响应客户端请求的主要角色外，能选择作为其他两个角色之一，作为检查点节点或是备份节点。角色在节点启动的时候指定。

检查点节点每隔一段时间就会组合现有的检查点和日记来产生新的检查点和一个空日记。检查点节点通常运行在不同的主节点主机上，因为它有和主节点一样的内存需求。它从主节点下载当前的检查点和日记文件，本地合并它们，并返回新的检查点给主节点。

创建周期性的检查点是保护文件系统元数据的方法之一。如果所有其他的命名空间映像和日记副本都不可用，那么系统可以从最近的检查点开始。

创建检查点，在新检查点上传到主节点时，让主节点截断日记的尾部。HDFS集群长时间运行且没有重启，在这段期间内日记会持续增长。如果日记变得非常大，那么丢失数据或日记出错的几率将会增加。并且，一个非常大的日记增加了主节点的重启时间。对于大集群，需要花费一小时来处理一个星期积累下来的日记。创建每日记录点是一个好的方法。

\subsection{备份节点}

HDFS最近一个引入的特色是备份节点。像检查点一样，备份节点能够创建周期性的检查点，但除了维护内存中的、最新的系统文件命名空间映像，映像一直和主节点的状态保持同步。

备份节点接受来自活动主节点的命名空间事务的日记流，将它们保存到存储目录，且将这些事务应用到内存中的命名空间映像。主节点把备份节点当作日记存储，就像对待存储目录中的日记文件一样。如果主节点失效，内存中备份节点的映像和磁盘的记录点将记录最新的命名空间状态。

存储节点可以创建新记录点而无需从活动的主节点下载记录点和日记,因为它在内存中已经有最新的命名空间映像。这使在备份节点上处理记录点更加高效，因为它只需要将命名空间保存到本地存储目录。

备份节点可以看作是一个只读的主节点。它包含了除块位置之外的所有文件系统元数据信息。它可以执行所有普通主节点不涉及修改命名空间和块位置信息的操作。备份节点提供了一种运行主节点却无需永久保存的选择，将命名空间状态持久化的责任交给备份节点。

\subsection{升级，文件系统快照}

在软件升级的过程中，系统出错的可能性会因为程序的漏洞或是人为的错误而有所增加。HDFS中快照的目的是为了在软件升级过程中，最小化对存储在系统中数据的潜在的危害。

快照机制让管理员永久保存文件系统的当前状态，以便如果升级导致数据丢失或出错时，能够回滚升级且让HDFS恢复到快照时的命名空间和存储状态。

在系统系统时由集群管理员选择创建快照（只能存在一个）。如果请求一个快照，那么主节点首先会读取检查点和日记，并且在内存中合并它们。然后将新的检查点和空日记写到新的位置，因此旧的检查点和日记保持不变。

在握手的过程中，主节点指导数据节点时候要创建本地快照。数据节点上的本地快照不能通过复制数据文件目录所创建，这样会使集群中每个数据节点的存储容量加倍。取而代之，每个数据节点创建一个存储目录的副本并将存在的块文件硬连接到其中。当数据节点删除一个块时，它只删除硬连接，且添加、修改块时使用写时复制技术。这样旧块副本原封不动保留在旧目录中。

集群管理员可以在重启系统时选择回滚HDFS到快照的状态。主节点恢复到建立快照时保存的检查点。数据节点恢复先前重命名的目录和开启一个后台进程来删除快照建立后新建的块副本。选这回滚后，没有前滚的选择。集群管理员可以通过命令系统丢弃快照来恢复快照占用的存储空间，因此结束软件升级。

系统演化可能会导致数据节点的检查点和日记文件，或是块副本文件数据表示的格式发生变化。布局版本识别数据表示格式，且永久保存在主节点和数据节点的存储目录中。在启动每个节点时，比较当前软件的布局版本和保存在存储目录中的本版，且自动将就格式转化成新格式。转化需要在有新软件布局本版的系统重启时，执行快照的新建命令。

HDFS为主节点和数据节点没有分离布局版本，因为新建快照必须是一个全集群范围的工作，而不是单一节点的事件。如果一个升级的主节点因为软件漏洞删除了它的映像，那么备份唯一的命名空间将仍会导致所有数据丢失，因为主节点将无法识别数据节点发送的块报告，且将命令删除它们。这种情况下回滚将恢复元数据，但数据本身将丢失。这时就需要一个候选的快照来防止大破坏。

\section{文件输入输出操作和副本管理}

\subsection{读写文件}

一个应用程序通过新建文件和将数据写到文件中的方式在HDFS上添加数据。在文件关闭之后，已经写入数据不能被更改或删除，除非重新打开这个文件添加新数据。HDFS实现了单一写者，多个读者的模式。

打开并写文件的HDFS客户端被授予该文件的一个租约；其他客户端不能写这个文件。执行写操作的客户端通过向主节点发送心跳来更新租期。当文件关闭时，租期被收回。

租期由软限制和硬限制共同约定。在软限制过期之前，写者将独占文件的访问权限。如果软限制过期了且客户端没能关闭文件或更新租期，另一个客户端就可以占用租期。如果在硬限制（1小时）过期之后，客户端仍然无法更新租期，HDFS就假设这个客户端已经退出并自动代替写者关闭文件，并恢复租期。读者的租期并不阻止其他客户端读取文件；一个文件可以同时被多个客户端读取。

一个HDFS文件包含很多块。当需要创建一个新块时，主节点分配一个唯一的块ID给新块，并决定哪一个数据节点的列表来保存块副本。数据节点建立管道the order of which最小化整个网络从客户端到最后一个数据节点的距离。比特流作为包序列被压进管道中。应用程序在客户端上将比特流写入第一个缓存。当一个包的缓存填满之后（通常是64KB），数据就被压入管道中。可以在收到前一个包的确认之前将下一个包压入管道。outstanding包的数量由客户端的outstanding 包窗口大小限制。

在数据写入一个HDFS文件后，在文件关闭之前，HDFS并不保证数据对新的读者可见。如果一个用户应用程序需要可见保证，它可以显式调用刷新（hflush）操作。然后当前的包将立即被压入管道，并且刷新操作会一直等待直到所有管道中所有数据节点确认成功传输包。所有在刷新操作之前写入入的数据将对读者可见。

如果没有错误发生，图2表示了一个有三个数据节点的管道和一个有5个包的块，块的构造经过如图2所示的三个阶段。图中，粗线表示数据包，虚线表示确认信息，细线表示控制建立和关闭管道的信息。垂直线表示客户端上的活动和三个数据节点，时间从上到下增加。从t0到t1是管道的建立阶段。t1到t2是数据流阶段，t1是第一个数据包发送的时间，t2是收到最后一个包的确认的时间。这有一个刷新操作传输第二个包。刷新指令和数据包一起传输，而不是分开的操作。最后t2到t3是这个块的管道关闭阶段。

在一个有上千个节点的集群中，节点失效（最常见的存储错误）每天都在发生。一个存储在数据节点的副本将因为内存、磁盘或网络的错误而出错。HDFS为HDFS文件的每个数据块产生和存储校验和。校验和由HDFS客户端在读取时校验，以帮助检查由客户端、数据节点或是网络引起的错误。当客户端新建一个HDFS文件时，它计算每个块的校验和序列并将数据发送给数据节点。数据节点在元数据文件中存储校验和，和块数据文件分开存储。当HDFS读取一个文件，每个块数据和校验和将传送给客户端。客户端计算为接受到的数据计算校验和并检验新计算出的校验和是否和它接受到校验和相匹配。如果不匹配，客户端通知主节点副本出错，然后从从另外的数据节点获取一个不同的块副本。

当一个客户端打开一个文件进行读取时，它从主节点获取块列表和每个块副本的位置。每个块的位置按它们离读者的距离排序。当读取一个块的内容时，客户端首先尝试最近的副本。如果读取尝试失败，客户端尝试序列中的下一个副本。目标数据节点不可用，或是在检查校验和时发现副本出错，可能导致读取失败。

HDFS允许客户端读取正在写入的文件。当读取一个正在写入的文件时，最后正在写入的块的长度对于数据节点是不可知的。这种情况下，在开始读取内容之前，客户向一个副本端请求最新的长度。

HDFS的输入输出是特别为像MapReduce这样需要大量顺序读写的批处理系统设计的。然而，为了支持像Scribe这样提供实时数据流给HDFS，或是像HBase这样提供随机的实时表访问的应用程序，在改进它的读写响应时间上做了很多努力。

\subsection{块放置}

对于一个大的集群，flat topology连接上所有节点是不现实的。一个通常的方法是将节点分散在多个机架上。一个机架上的节点共享一台交换机，且机架交换机由一台或多台核心交换机连接。不同机架上的两个节点的通信必须通过多台交换机。在大多数情况下，同机架节点之间的网络带宽大于不同机架节点之间的网络带宽。图3描述了有两个机架的集群，每机架包含3个节点。

HDFS通过两个节点之间的距离估计它们之间的网络带宽。假设从一个节点到它的父节点的距离是1。通过把把两个节点到他们最近的共同祖先的距离相加可以计算两个节点之间的距离。两个节点之间的距离越短，意味着它们可用于传输数据的带宽越大。

HDFS允许管理员设置脚本来以一个节点的地址作为输入，返回节点的机架信息。主节点是解决各个数据节点机架位置的中心。当一个数据节点注册到主节点时，主节点运行一个配置脚本来决定该节点属于哪个机架。如果没有配置脚本，主节点就假设所有节点都属于一个默认的机架。

副本的放置是HDFS数据稳定性和读写性能的关键。一个好的副本放置策略能提高数据稳定性、可用性和网络带宽利用率。目前HDFS提供可配置的块放置策略接口，这样用户和研究员就可以实验和测试哪一个策略对于他们的应用程序是最优的。

默认的HDFS块放置策略提供一个最小化写代价和最大化数据稳定性、可用性和总的读带宽之间的tradeoff。当新建一个新块时，HDFS将第一个副本放置在读者所在的节点，第二和第三副本放置在两个不同机架上的不同节点上，剩下的放置在随机的节点上，但是要求在副本的数量少于机架的两倍时，一个节点上放置不多于一个副本，同一机架上放置不多于二个副本。放置第二和第三个副本在不同机架上的选择更好地将一个文件的块副本分配到整个集群中。如果头两个副本被放置在同一机架上，那么对于任何文件，它2/3的块副本都将放置在同个机架上。

在选择所有目标节点后，节点按照它们到第一个副本从近到远的顺序被组织成管道。数据按这样的顺序被压入管道。对于读取，主节点首先检查客户端主机是否位于这个集群中。如果是，那么将块位置按它到客户端的距离由近到远的顺序返回给客户端。从数据节点中按照优先顺序度读取块。（MapReduce应用程序通常运行在集群节点上，但只要主机能连接上主节点和数据节点，那么它就能运行HDFS客户端。）

这种策略减少中间机架和中间节点写传输量，且通常能提高写性能。因为机架失效的可能性远远低于一个节点失效的可能性，这中策略没有影响数据的可靠性和可用性保证。在三个副本的通常情况下，在读数据时能减少使用的总网络带宽，因为一个块只放置在两个不同的机架而不是三个。

默认的HDFS副本放置策略可以总结如下：
\begin{itemize}
\item没有数据节点包含多余一个任意块的副本。
\item没有机架包含多余两个相同块的副本，在集群中提供足够的机架。
\end{itemize}
\subsection{副本管理}

主节点努力确保每个块总是拥有一定数量的副本。当接受到一个来自数据节点的块报告时，主节点检测一个块副本数是低于或高于标准数量。当一个块副本数过多时，主节点选择一个副本进行删除。主节点首先倾向于不减少持有副本的机架的数量，其次倾向于从可用空间最少的数据节点中删除副本。目的在于平衡数据节点之间的存储空间利用率，同时不降低块的可用性。

当一个块副本数量变得过少时，它就被放入复制优先队列。只有一个副本的块拥有最高优先级，同时副本数量大于它复制因子的2/3的块优先级最低。一个后台线程周期性地扫描复制队列的队头来决定把新副本放在哪。块的复制遵循相识于新块放置的策略。如果现有的副本数为1，HDFS将下一个副本放置在不同的机架上。在块有2个副本的情况下，如果连个副本在同一机架上，那么第三个副本就放在一个不同机架上；否则，第三个副本就放在现有副本的机架的一个不同节点上。目的是为了减少创建新副本的代价。

主节点还确保并非所有块副本都位于一个机架上。如果主节点发现一个块的副本都在同一机架上，那主节点就把该块当作副本数过少，同时使用前面提到的相同的块放置策略复制块到一个不同的机架上。在一个主节点接受到新副本已经创建的通知后，块变为副本过多的状态。之后主节点将决定删除一个旧副本，因为过多复制策略倾向于减少机架的数目。

\subsection{平衡器}

HDFS块放置策略并不计算数据节点的磁盘空间利用率。这是为了避免新数据（更可能是引用的数据）成为数据节点的一个子集。因此在数据节点中数据可能不总是一致地放置。当新结点加入集群时也会发生不平衡。

平衡器是一个平衡HDFS集群上的磁盘使用的工具。把一个阈值作为输入参数，阈值的范围是0到1之间。如果每个数据节点，节点的使用率（节点上使用空间占总空间的比率）和整个集群的使用率（集群中使用空间占总空间的比率）的差值不超过阈值，那么一个集群就达到平衡。

这个工具被设置成一个可以被集群管理员运行的应用程序。它迭代地将副本从高利用率的数据节点移动到低利用率的节点。平衡器的一个关键要求是维护数据的可用性。当选择一个副本进行移动和决定它的目标位置时，平衡器保证这个决定不会减少副本或是机架的数量。

平衡器通过最小化中间机架数据拷贝来优化平衡进程。如果平衡器决定一个副本A需要移动到不同的机架，且目标位置恰好有相同块的副本B，那么数据将从副本B除拷贝而不是副本A处。

第二个配置参数限制了再平衡操作消耗的带宽。所允许的带宽越大，集群达到平衡状态的速度越快，但是和应用程序进程竞争得越厉害。

\subsection{块扫描器}

每个数据节点运行一个块扫描器周期性地扫描它的块副本且检验存储校验和是否和块数据匹配。在每个扫描过程中，块扫描器为了在配置的时间内完成检验而调整读带宽。如果客户端读取一个完整的块且校验和验证成功，它将通知数据节点。数据节点就把它作为副本的检验。

每个块的校验时间存放在可读日志文件。在任何时间，最多只有两个文件在最高层的数据节点目录中，目前和之前的日志。新的检验情况将追加到目前文件中。每个数据节点相应地有一个在内存中按副本检验时间排序的扫描列表。

任何时候一个读客户端或是块扫描器检测到一个错误块，它将通知主节点。主节点标记副本出错，但并不立即删除副本。而是开始复制一个好的块副本。只有当好的副本数量达到了块复制因子时，才将出错副本删除。这个策略是为了尽可能久地保存数据。所以即使所有块副本都出错，这个策略能让用户从出错副本中取回数据。

\subsection{退役}
集群管理员通过列出允许注册的节点的主机地址和不允许注册的节点的主机地址来指定哪些节点可以加入集群。管理员可以命令系统重新评估这些包含和排除列表。一个集群现有成员变成排除状态时，就被标记为退役。只要一个数据节点标记为退役，它就不会作为副本目标位置被选中，但它将继续服务读请求。主节点开始计划复制块到其他数据节点。只要主节点发现退役数据节点所有块被复制，那么数据节点就进入退役状态。之后它就可以安全地从集群中移除而不危害任何数据的可用性。

\subsection{集群之间数据拷贝}

当处理大数据集时，复制数据近出HDFS集群是令人畏惧的。HDFS提供了一个叫作DistCp的工具用于大数据在集群间或集群内的并行拷贝。它是一个MapReduce任务。每个map任务复制源数据的一部分到文件系统中的目标位置。MapReduce框架自动处理并行任务计划、错误检测和恢复。

\section{在Yahoo!的实践}

在雅虎的大型HDFS集群包括大约3500个节点。一个典型的集群节点的配置如下：
\begin{itemize}
\item双核Xeon处理器,频率2.5ghz
\item红帽企业版Linux服务器5.1版
\item Sun Java JDK 1.6.0 13-b03
\item4个直接连接的SATA硬盘（每个1terabyte）
\item16G内存
\item1GB以太网
\end{itemize}

70\%的磁盘空间分配给HDFS。剩余的空间留给操作系统（红帽Linux）、日志和存储map任务的输出流。（MapReduce的中间数据并不存放在HDFS。）40个节点在同一个机架上共享一个IP交换机。机架交换机和8台核心交换机台台相连。核心交换机提供机架和集群外资源连接。每个集群，主节点和备份节点主机特别配备了最多64GB的内存；应用程序任务永远不会分配给这些主机。总体上，一个3500个节点的集群有9.8PB的存储空间可用，相当于3.3PB应用程序存储空间的块复制3次。作为一个方便的近似，一千台节点代表1PB的应用程序存储。在HDFS被使用的这些年（和将来），作为集群节点的主机将从改进的技术中获益。新集群节点总是拥有更快的处理器，更大的磁盘空间和更大的内存。较慢、较小的节点将从集群中回收或撤离，然后用于Hadoop的研发和测试。如何准备数据节点的选择很大程度上是一个经济采购计算能力和存储空间的问题。HDFS不强制使用一个特定的计算能力存储空间比率，或是给连接到集群的节点设定一个存储容量的限制。

举一个大型集群（3500个节点）的例子，有近乎6千万个文件。这些文件有6.3千万个块。每个块通常复制三次，每个数据节点有54000个块副本。集群中每天用户应用程序会新建2百万个新文件。在Yahoo!的Hadoop集群中，25000个节点提供了25PB的在线数据存储。从2010年开始，这是Yahoo!一个中型但不断发展的数据处理框架的部分。2004年，Yahoo!开始调查有分布式文件系统的MapReduce编程。2006年，Apache的Hadoop项目成立。2006年底，Yahoo!已经将Hadoop用于内部使用并且拥有300个节点的集群用于开发。从此HDFS成为Yahoo!后台管理系统不可分割的一部分。HDFS标志性的应用程序是网站地图的产生，这是一个万维网的索引，是搜索至关重要的部分（75小时的时间，产生了500terabyte的MapReduce中间数据，300terabyte的总输出）。更多的应用程序迁移到了Hadoop上，尤其是那些分析用户行为并建模的程序。

成为Yahoo!的技术套件的关键组成，意味着在解决研究项目和管理很多petabyte的企业数据的问题之间存在差异。更重要的是稳健性和数据持久性的问题。但是经济的性能、用户社区成员之间共享资源的准备和系统操作者的简便管理也很重要。

\subsection{数据持久性}

数据复制三次是一个防止因不相关节点失效引起数据丢失的健壮性措施。采用这种方法，Yahoo!从没有丢失过一个块。对于大型集群，一年之中丢失块的可能性小于0.005。关键在于能理解节点每个月有0.8\%的几率失效。（即使节点最终被恢复了，但却没办法恢复它之前保存的数据。）所以对之前描述的大型集群样本，每天会丢失1或2个节点。相同的集群会在2分钟左右重新新建54000个失效节点上的块副本。（重新复制很快，因为这是一个随着集群规模而伸缩的并行问题。）几个节点在两分钟之内失效而导致一些块副本丢失的可能性相当小。

相关节点的失效则是一个不同的威胁。在这方面，最常见出现的错误是机架或核心交换机的失效。HDFS可以容许丢失一台机架交换机（每个块还有副本在其他的机架上）。一台核心交换机的失效可能有效地断开一片集群和多个机架的连接，这种情况下一些块将会不可用。在两种情况下，修复交换机可将不可用的副本恢复到集群中。另一种相关失效是集群意外或有意断电。如果断电跨越了机架，很有可能一些块将变得不可用。但是恢复供电可能不能修复丢失数据，因为有1.5\%的节点不能在全加电重启中存活。跟据统计和实践，一个大集群将在加点重启过程中丢失一小撮块。（在几星期内，用每次有意重启一个节点的策略来鉴别不能在重启中存活且还没被检测到的节点。）

除此之外全部节点失效，那么存储的数据可能出错或丢失。块扫描器每两星期扫描以此大集群中的所有块，并且在这个过程中发现约20个坏的副本。

\subsection{照顾下议院}

随着HDFS的使用日益增长，文件系统本身必须引入在一个大型且不同的用户社区中共享资源的方法。第一个这样的特点是权限框架，它模仿了Unix文件和目录的权限管理。在这个框架中，文件和目录对于所有者、文件或目录所有者所在群体的其他成员和所有其他用户有不同的访问权限。Unix（POSIX）和HDFS之间的根本不同在于在HDFS上普通文件只拥有“可执行”权限或“粘着”位。

在目前的框架中，用户标识弱：主机说你是谁就是谁。当访问HDFS时，应用程序客户端简单地在本地操作系统查询用户标识和群体成员。一个更强标识的模型正处于研发之中。在新的框架中，应用程序客户端必须向名字系统出示从可信任源获取的凭据。不同凭据的管理是可能的；最初实现将使用Kerberos。用户应用程序可以使用相同的框架来确认名字系统也有可信任标识。并且名字系统还可以要求从集群中的每个数据节点获取凭据。

总的可用数据存储空间由数据节点的数量和为每个节点配备的空间所确定。HDFS的早期实验中证明需要一些方法来强制执行资源块用户社区分配的策略。不仅需要强制执行公平共享，在用户应用程序可能涉及在上千台主机上写数据时，避免应用程序偶然耗尽资源也很重要。对于HDFS，因为系统元数据永远在内存中，命名空间的大小（文件和目录的数量）也是一项有尽的资源。为了管理存储空间和命名空间资源，可能给每个目录分配一个限额，限额也由子树中总的文件和目录数决定。

虽然HDFS的架构假设大多数应用程序会将大量数数据流作为输入，但是MapReduce程序框架倾向于产生很多小的输出文件（每个reduce任务产生一个），给命名空间资源造成更大的压力。方便起见，目录子树压缩成一个Hadoop压缩文件。一个HAR文件类似于tar、JAR或是Zip文件，但是文件系统的操作可以把个别的文件打包压缩，且HAR文件可以透明地用作MapReduce任务的输入。

\subsection{基准程序}

HDFS的设计目标之一是为大数据集提供很高的输入输出带宽。有三种测量标准来检测这个目标。
\begin{itemize}
\item从人造基准观测到的带宽是多少？
\item在一个最少用户任务的生产集群中观测到的带宽是多少？
\item一个最精心构造的大型用户应用程序能获得得的带宽是多少？
\end{itemize}
这里的统计报告是从至少3500个节点的集群中获取的。在这种规模下，总带宽和节点数量线性相关，所以我们感兴趣的统计数据是每个节点的带宽。这个基准程序作为Hadoop codebase的一部分是可用的。

DFSIO基准程序衡量了读、写和追加操作的平均数量。DFSIO是一个可用的应用程序，它作为Hadoop分布式系统的一部分。MapReduce程序从大文件中读/写/追加随机的数据，或是将随机数据读/写/追加到大文件中。这个任务中的每个Map任务在不同的文件上执行相同的操作，传输相同数量的数据，并且将它的传输速率报告给单个reduce任务。然后reduce任务汇总这些测量。这个测试不需要争用其他应用程序资源就可以运行，并且会选择和集群规模相称的map任务数量。它设计只是用来测量数据传输时的性能，并且排除任务计划、启动和reduce任务造成的开销。
\begin{itemize}
\item DFSIO读速度：66MB/s每节点
\item DFSIO写速度：40MB/s每节点
\end{itemize}
对于一个生产集群，读取和写入的比特数将报告给一个metrics采集系统。这些平均值将被接管几个星期，且通过上百个独立独立用户表示集群的利用率。平均来看，任何时刻1或2个应用程序任务占用1个节点（少于可用的处理器核数）。
\begin{itemize}
\item繁忙集群的读速度：1.02MB/s每节点
\item繁忙集群的写速度：1.09MB/s每节点
\end{itemize}

表2

在2009年年初，Yahoo!参加了Gray Sort竞赛。这个任务的目的是测试系统移动文件的能力所能承受的压力（和排序没有关联）。竞赛方面意味着表2的结果是在现有设计和硬件条件下，最好的用户应用程序所能达到的结果。最后一列的输入输出速度是从HDFS读取输入和将输出写入HDFS的总和。在第二行，虽然HDFS的速率减少，但每个节点总的输入输出将近翻了一倍，因为对于大数据集（betabyte规模），MapReduce中间结果也必须写入磁盘和从磁盘中读取。在较小规模的测试中，不需要将MapReduce的中间结果写入磁盘；它们缓存在这个任务的内存中。

大型的集群需要HDFS主节点支持大集群所期望达到的客户端操作数。NNThroughput基准程序是一个单一节点的进程，它开启主节点应用程序且相同节点上运行一些列客户端线程。每个客户端线程通过直接调用主节点实现这些操作的方法重复执行相同的主节点操作。基准程序测量主节点每秒执行的操作数。这个基准程序设计上避免了RPC连接和序列化造成的通信的外开销，因此在本地运行客户端而不是从不同节点上远程运行。这里表3提供了主节点净性能的上限。

\section{后续工作}

这部分展示一些Yahoo的Hadoop团队正在考虑的一些后续工作；Hadoop是一个开源项目意味着新的功能和改变大部分由Hadoop开发社区决定。

当主节点失效时，将导致Hadoop集群不可用。考虑到Hadoop主要用于批处理系统，重启主节点已经是一个令人满意的方法。然而，我们已经像自动的失效备份发展。目前的备份节点接受所有来自主要的主节点的事务。这将允许将失效备份转移到一个可用的或甚至是繁忙的备份节点上，如果我们发送块报告到主要的主节点和备份节点的话。除Yahoo!之外的一些Hadoop用户已经尝试手动失效备份的实验。我们的计划是使用Zookeeper，Yahoo的分布式一致技术来建立一个自动的失效备份方案。

主节点的伸缩性已经成为一个关键。因为主节点存有所有的内从中的命名空间和块位置，主节点堆的大小已经限制了文件的数量和块地址的数量。主节点主要面临的挑战是当内存的使用接近上限时，主节点会因为Java的垃圾回收机制而变得响应迟钝，并且有时需要重启。虽然我们鼓励我们的用户创建大文件，但这还没发生，因为它要求改变应用程序的行为。我们已经添加了一些配额来管理使用情况，且提统一个压缩工具。然而这些没有根本上解决伸缩性的问题。

我们近期的伸缩性解决方案是允许多个命名空间（和主节点）在集群内共享物理存储空间。我们通过块池（block pool）标识符扩展块ID作为前缀。块池和SAN存储系统 的LUNS相识，且有块池命名空间的大小和文件系统容量相识。

这个方法相当简单，而且需要对系统的改变最少。除伸缩性之外，它还提供了一些其他优势：它分离了不同应用程序集的命名空间且提高了集群的整体可用性。它还产生了块存储的抽象，让其他服务能用不同的命名空间架构来使用块存储服务。我们计划去探索其他伸缩性的方法，例如只将部分命名空间保存在内存中和将来主节点真正的分布实现。特别地，应用程序会产生少数的大文件的假设是有瑕疵的。如之前所提到的，改变应用程序行为是很困难的。此外，我们见过新型的HDFS应用程序需要存储大量的小文件。

多个独立命名空间的不足之处在于管理成本，特别是如果命名空间的数量特别多时。我们也计划使用应用程序或以任务为中心的命名空间而不是集群为中心的命名空间—这类似于80年代末90年代初，在分布式系统中用于处理远程执行的按照进程的命名空间。

目前我们的集群少于4000个节点。我们相信我们可以通过上述解决方案扩充成更大的集群。然而，我们相信多个集群将更加的明智合理，而不是单一的大集群（也就是说3个6000个节点的集群而不是1个18000个节点的集群），因为它允许提高了可用性和独立性。为了达到这个最终目标，我们计划在集群之间提供更多的协作。比如缓存远程访问文件或是当文件集跨集群复制时，减少块的复制因子。

\section*{致谢}
\addcontentsline{toc}P{section}{致谢}
我们想要感谢现在或之前Yahoo! HDFS团队的所有成员为建立这个文件系统所作出的努力。我们想要感谢所有的Hadoop委员会成员和合作者的宝贵贡献。Corinne Chandel为这篇文章绘制了图表。

